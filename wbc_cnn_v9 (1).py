# -*- coding: utf-8 -*-
"""WBC CNN v9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zy3S5PbIbLDyFnmuh7L0TYb96HAcAbfe
"""



"""# Introduction
Develop a CNN model to classify images of white blood cells. This will lessen the workload of manual differentials of the lab techs for abnormal patient results. Confirmation of correct classification is still the responsibility of the tech before releasing patient results.

All errors of this project are saved to show the process of editing the code.

**DISCLAIMER**: This model is used for informational purposes only and not a substitute for professional medical advice.

Patient confidentiality is secured and viewed by authorized persons only. Data is erased after 180 days.

## 1. Unzip the dataset and import modules

Extract the images from the `raabin_wbc.zip` file in Google Drive.

**IMPORT** - The first step is to import necessary modules and define the file path and extraction directory.
"""

import zipfile
import os

zip_file_path = '/content/drive/MyDrive/raabin_wbc.zip'
extracted_dir = '/content/raabin_wbc_extracted'

# Create the extraction directory if it doesn't exist
if not os.path.exists(extracted_dir):
    os.makedirs(extracted_dir)

"""**1.2**
Open the zip file and extract its contents to the specified directory.


"""

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir)

"""**NOTES** Copilot could not find the file because it was not in the correct folder. Dragging the .zip file to the correct directory resolved the error.

## 2. Load and preprocess the data

Load the images and labels, preprocess them, and split the data into training and testing sets as per the instructions.
"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

extracted_dir = '/content/raabin_wbc_extracted'
image_files = [os.path.join(extracted_dir, f) for f in os.listdir(extracted_dir) if f.endswith('.jpg')]

images = []
labels = []
img_size = (128, 128)

for img_path in image_files:
    img = cv2.imread(img_path)
    img = cv2.resize(img, img_size)
    img = img.astype('float32') / 255.0  # Normalize
    images.append(img)

    # Extract label from filename (assuming filename format is 'label_imageid.jpg')
    label = os.path.basename(img_path).split('_')[0]
    labels.append(label)

images = np.array(images)
labels = np.array(labels)

# Convert labels to numerical format (one-hot encoding)
unique_labels = np.unique(labels)
label_map = {label: i for i, label in enumerate(unique_labels)}
numerical_labels = np.array([label_map[label] for label in labels])
one_hot_labels = to_categorical(numerical_labels, num_classes=len(unique_labels))

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(images, one_hot_labels, test_size=0.2, random_state=42)

print(f"Number of images loaded: {len(image_files)}")
print(f"Shape of images array: {images.shape}")
print(f"Shape of one-hot labels array: {one_hot_labels.shape}")
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""The error indicates that the `numerical_labels` array is empty, which means no images were loaded or labels extracted. This could be due to an incorrect path to the extracted directory or no image files being found.


"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

extracted_dir = '/content/raabin_wbc_extracted'
image_files = [os.path.join(extracted_dir, f) for f in os.listdir(extracted_dir) if f.endswith('.jpg')]

if not image_files:
    print(f"No JPG files found in the directory: {extracted_dir}")
else:
    print(f"Found {len(image_files)} image files. First 5: {image_files[:5]}")

images = []
labels = []
img_size = (128, 128)

for img_path in image_files:
    img = cv2.imread(img_path)
    if img is not None:
        img = cv2.resize(img, img_size)
        img = img.astype('float32') / 255.0  # Normalize
        images.append(img)

        # Extract label from filename (assuming filename format is 'label_imageid.jpg')
        label = os.path.basename(img_path).split('_')[0]
        labels.append(label)
    else:
        print(f"Warning: Could not read image file: {img_path}")


if not images:
    print("No images were successfully loaded.")
else:
    images = np.array(images)
    labels = np.array(labels)

    # Convert labels to numerical format (one-hot encoding)
    unique_labels = np.unique(labels)
    label_map = {label: i for i, label in enumerate(unique_labels)}
    numerical_labels = np.array([label_map[label] for label in labels])
    one_hot_labels = to_categorical(numerical_labels, num_classes=len(unique_labels))

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(images, one_hot_labels, test_size=0.2, random_state=42)

    print(f"Number of images loaded: {len(image_files)}")
    print(f"Shape of images array: {images.shape}")
    print(f"Shape of one-hot labels array: {one_hot_labels.shape}")
    print(f"Shape of X_train: {X_train.shape}")
    print(f"Shape of X_test: {X_test.shape}")
    print(f"Shape of y_train: {y_train.shape}")
    print(f"Shape of y_test: {y_test.shape}")

"""The previous attempt failed because no image files were found in the specified directory. This could be because the extraction process did not create the expected directory structure or because the files have a different extension.


"""

import os

extracted_dir = '/content/raabin_wbc_extracted'

# List contents of the extracted directory
if os.path.exists(extracted_dir):
    print(f"Contents of {extracted_dir}:")
    for item in os.listdir(extracted_dir):
        print(item)
else:
    print(f"Directory not found: {extracted_dir}")

"""The directory listing shows that the images are not directly in `/content/raabin_wbc_extracted`, but are likely within the `Train`, `TestA`, and `TestB` subdirectories. Copilot updated the code to recursively search for image files within these subdirectories and then proceed with loading, preprocessing, and splitting the data.


"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

extracted_dir = '/content/raabin_wbc_extracted'
image_files = []
for root, _, files in os.walk(extracted_dir):
    for file in files:
        if file.endswith('.jpg'):
            image_files.append(os.path.join(root, file))

if not image_files:
    print(f"No JPG files found in the directory: {extracted_dir} or its subdirectories.")
else:
    print(f"Found {len(image_files)} image files.")

images = []
labels = []
img_size = (128, 128)

for img_path in image_files:
    img = cv2.imread(img_path)
    if img is not None:
        img = cv2.resize(img, img_size)
        img = img.astype('float32') / 255.0  # Normalize
        images.append(img)

        # Extract label from filename (assuming filename format is 'label_imageid.jpg')
        # Need to handle potential differences in filename format across subdirectories
        # Assuming the label is the first part of the filename before an underscore
        label = os.path.basename(img_path).split('_')[0]
        labels.append(label)
    else:
        print(f"Warning: Could not read image file: {img_path}")

if not images:
    print("No images were successfully loaded.")
else:
    images = np.array(images)
    labels = np.array(labels)

    # Convert labels to numerical format (one-hot encoding)
    unique_labels = np.unique(labels)
    label_map = {label: i for i, label in enumerate(unique_labels)}
    numerical_labels = np.array([label_map[label] for label in labels])
    one_hot_labels = to_categorical(numerical_labels, num_classes=len(unique_labels))

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(images, one_hot_labels, test_size=0.2, random_state=42)

    print(f"Number of images loaded: {len(images)}")
    print(f"Shape of images array: {images.shape}")
    print(f"Shape of one-hot labels array: {one_hot_labels.shape}")
    print(f"Shape of X_train: {X_train.shape}")
    print(f"Shape of X_test: {X_test.shape}")
    print(f"Shape of y_train: {y_train.shape}")
    print(f"Shape of y_test: {y_test.shape}")

"""## 3. Build the cnn model

Define the architecture of the CNN model using a deep learning framework like TensorFlow or PyTorch.

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

num_classes = len(label_map)
input_shape = (img_size[0], img_size[1], 3) # Assuming color images (3 channels)

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

"""## 4. Train the model

Train the compiled CNN model using the training data and validate with the test data, storing the training history.

"""

epochs = 15 # Increased epochs for potentially better convergence
batch_size = 64 # Using a batch size of 64

history = model.fit(X_train, y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=(X_test, y_test))

"""## 5. Evaluate the model

Evaluate the trained model's performance on the test set and print the results.

"""

loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

"""The model object is not defined, likely because the kernel was reset or the variable was not preserved. Copilit will rebuild the model and then evaluate it.

**NOTES:** A runtime error occurred in the next step.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Assuming num_classes and img_size are still available from previous steps
num_classes = len(label_map) # This variable needs to be defined if kernel reset
img_size = (128, 128) # This variable needs to be defined if kernel reset
input_shape = (img_size[0], img_size[1], 3)

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Now evaluate the model
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

"""The variables `label_map`, `X_test`, and `y_test` are not defined, indicating that the kernel has been reset and the data preprocessing step needs to be re-run to create these variables.


"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam


extracted_dir = '/content/raabin_wbc_extracted'
image_files = []
for root, _, files in os.walk(extracted_dir):
    for file in files:
        if file.endswith('.jpg'):
            image_files.append(os.path.join(root, file))

if not image_files:
    print(f"No JPG files found in the directory: {extracted_dir} or its subdirectories.")
else:
    print(f"Found {len(image_files)} image files.")

images = []
labels = []
img_size = (128, 128)

for img_path in image_files:
    img = cv2.imread(img_path)
    if img is not None:
        img = cv2.resize(img, img_size)
        img = img.astype('float32') / 255.0  # Normalize
        images.append(img)

        # Extract label from filename (assuming filename format is 'label_imageid.jpg')
        label = os.path.basename(img_path).split('_')[0]
        labels.append(label)
    else:
        print(f"Warning: Could not read image file: {img_path}")

if not images:
    print("No images were successfully loaded.")
else:
    images = np.array(images)
    labels = np.array(labels)

    # Convert labels to numerical format (one-hot encoding)
    unique_labels = np.unique(labels)
    label_map = {label: i for i, label in enumerate(unique_labels)}
    numerical_labels = np.array([label_map[label] for label in labels])
    one_hot_labels = to_categorical(numerical_labels, num_classes=len(unique_labels))

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(images, one_hot_labels, test_size=0.2, random_state=42)

    print(f"Number of images loaded: {len(images)}")
    print(f"Shape of images array: {images.shape}")
    print(f"Shape of one-hot labels array: {one_hot_labels.shape}")
    print(f"Shape of X_train: {X_train.shape}")
    print(f"Shape of X_test: {X_test.shape}")
    print(f"Shape of y_train: {y_train.shape}")
    print(f"Shape of y_test: {y_test.shape}")

    # Now, define and compile the model
    num_classes = len(label_map)
    input_shape = (img_size[0], img_size[1], 3)

    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer=Adam(),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # Train the model (re-training is necessary after kernel reset)
    epochs = 15
    batch_size = 64

    history = model.fit(X_train, y_train,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(X_test, y_test),
                        verbose=0) # Set verbose to 0 to reduce output during re-training

    # Finally, evaluate the trained model
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Loss: {loss:.4f}")
    print(f"Test Accuracy: {accuracy:.4f}")

"""## Summary:

### Data Analysis Key Findings

* A total of 16,633 image files were found, loaded, and preprocessed from the extracted dataset.
* The images were resized to 128x128 pixels and normalized.
* The dataset was split into training (80%) and testing (20%) sets.
* The CNN model architecture included three convolutional layers with increasing filter sizes (32, 64, 128), followed by max-pooling layers, a flatten layer, a dense layer (128 units), a dropout layer (0.5), and a final dense output layer with a softmax activation.
* The model was trained for 15 epochs with a batch size of 64.
* The model achieved a test accuracy of approximately 0.9815 and a test loss of approximately 0.0648.

### Insights or Next Steps

* The high test accuracy suggests the model is performing well on the white blood cell classification task.
* Further steps could include exploring techniques to improve model performance, such as data augmentation, hyperparameter tuning, or experimenting with different CNN architectures, and visualizing the model's predictions and potential misclassifications.

**Insights**

While the process of designing this code was straightforward, it was not without the many errors and small fixes I had to edit ino order for the model to work.

**Conclusion**

As a medical laboratory technician myself, I have seen first hand what AI and machine learning can do when embedded into existing technology. The Cellavision is a blood analyzer capable of staining and reading slides with amazing accuracy. It is designed to take account of staining variables and system parameters. However, though the Cellavision is pre-trained on an vast data library with millions of patient samples, there is still the need for human intervention.The tech still has to review the labelled cells before resulting anything to the physician. This simple model demonstrated in this project is proof that even a small CNN can do the job. I have hope that intelligent microscopy will set new standards and manual differentials will one day be a thing of the past.
"""