{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8410d0f",
      "metadata": {
        "id": "e8410d0f"
      },
      "source": [
        "# White blood cell classifier\n",
        "\n",
        "Concept:\n",
        "\n",
        "Build a model that can classify different types of white blood cells through images.\n",
        "\n",
        "Rationale: Successful classification of cells leads to a more efficient way of doing manual differentials\n",
        "\n",
        "Feasibility: A basic model can be built within a weekend using readily available Python libraries.\n",
        "\n",
        "Relevance: AI Classification is used in analyzers that are able to review stained blood slides. This advanced technology in healthcare lessens the workload of the lab tech who must review each abnormal slide and count the cells manually.\n",
        "\n",
        "Data-centric: This application relies heavily on large datasets of white blood cells in order to be trained to identify them confidently."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  Setup and Imports\n",
        "\n",
        "First, we need to Install and  import the necessary libraries. Each library serves a specific purpose  "
      ],
      "metadata": {
        "id": "Uw2tYGIGANzz"
      },
      "id": "Uw2tYGIGANzz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f1a99c",
      "metadata": {
        "id": "87f1a99c",
        "outputId": "1bd2439e-3152-4300-dd4a-981a71d0c43c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\jelly\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\jelly\\anaconda3\\lib\\site-packages (0.19.1)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\jelly\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
            "Requirement already satisfied: fsspec in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
            "Requirement already satisfied: sympy in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torch) (1.8)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torch) (2.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from torchvision) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from networkx->torch) (5.0.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jelly\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "True\n",
            "11.8\n",
            "1\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install minimal required packages (run once)\n",
        "# In some environments, you may already have these packages installed.\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "import os, zipfile, random, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.version.cuda)\n",
        "print(torch.cuda.device_count())\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print('Using device:', DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WENvbLJuC2_J"
      },
      "id": "WENvbLJuC2_J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {},
        "id": "hiLAKbnQ_wge"
      },
      "source": [
        "# 3. Data Preparation\n",
        "\n"
      ],
      "id": "hiLAKbnQ_wge"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4f0816",
      "metadata": {
        "id": "8d4f0816",
        "outputId": "82b33220-8423-45dc-9f7a-8b4dcddd48b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already extracted to raabin_raw\n",
            "Top-level folders: ['TestA', 'TestB', 'Train']\n",
            "Image counts (possible splits): {'Train': 10175, 'train': 10175, 'TestA': 4339, 'TestB': 2119, 'testA': 4339, 'testB': 2119}\n",
            "Sample images (first 10):\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163027_0.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163055_0.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163055_1.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163055_2.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163545_0.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163656_0.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163835_0.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_163835_1.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_164040_0.jpg\n",
            " - raabin_raw\\TestA\\TestA\\Basophil\\20190526_164638_0.jpg\n"
          ]
        }
      ],
      "source": [
        "# Unzip uploaded Raabin dataset and inspect structure\n",
        "ZIP_PATH = Path('/mnt/data/raabin_wbc.zip')\n",
        "EXTRACT_DIR = Path('raabin_raw')\n",
        "\n",
        "if not EXTRACT_DIR.exists():\n",
        "    print('Extracting', ZIP_PATH)\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zf:\n",
        "        zf.extractall(EXTRACT_DIR)\n",
        "else:\n",
        "    print('Already extracted to', EXTRACT_DIR)\n",
        "\n",
        "# List top-level dirs\n",
        "top = [p for p in EXTRACT_DIR.iterdir() if p.is_dir()]\n",
        "print('Top-level folders:', [p.name for p in top])\n",
        "\n",
        "# Count images in Train/TestA/TestB-like folders\n",
        "image_exts = ('.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp')\n",
        "counts = {}\n",
        "for name in ['Train','train','TestA','TestB','test','testA','testB']:\n",
        "    p = EXTRACT_DIR / name\n",
        "    if p.exists():\n",
        "        imgs = list(p.rglob('*'))\n",
        "        img_count = sum(1 for f in imgs if f.is_file() and f.suffix.lower() in image_exts)\n",
        "        counts[name] = img_count\n",
        "print('Image counts (possible splits):', counts)\n",
        "\n",
        "# Show a few sample image paths\n",
        "samples = []\n",
        "for p in EXTRACT_DIR.rglob('*'):\n",
        "    if p.is_file() and p.suffix.lower() in image_exts:\n",
        "        samples.append(str(p)[:200])\n",
        "        if len(samples) >= 10: break\n",
        "print('Sample images (first 10):')\n",
        "for s in samples: print(' -', s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1 Splitting data\n",
        "Before training the model, we will seperate the images into training and validation sets. It will help to see how well the model can identify new images of these cells.\n",
        "\n",
        "The training set will be a larger set of data vs the validation set. The validation set is not used in training and will be used for evaluation of the model's performance. This will test to see if the model learns general patterns instead of memorizing the images."
      ],
      "metadata": {
        "id": "PWTF9rKUED_0"
      },
      "id": "PWTF9rKUED_0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61dd23e3",
      "metadata": {
        "id": "61dd23e3",
        "outputId": "4c0bbb3d-0d27-4dbd-e7f7-a6c4fa4eecd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data dir already exists: wbc_raabin_split\n",
            "Archive folders used -> train: raabin_raw\\Train val: raabin_raw\\TestA test: raabin_raw\\TestB\n",
            "Using Train/TestA/TestB split from archive (no random splitting).\n",
            "Counts per split/class (sample):\n",
            "test/Test-B: 2119\n",
            "train/Train: 10175\n",
            "val/TestA: 4339\n"
          ]
        }
      ],
      "source": [
        "# Create standardized split directories: train/ val/ test based on Train / TestA / TestB in archive.\n",
        "DATA_DIR = Path('wbc_raabin_split')\n",
        "if DATA_DIR.exists():\n",
        "    print('Data dir already exists:', DATA_DIR)\n",
        "else:\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Map archive folders to our splits (prefer exact names found in the archive)\n",
        "archive_train = None\n",
        "archive_val = None\n",
        "archive_test = None\n",
        "\n",
        "# Prefer 'Train' for training, 'TestA' for val, 'TestB' for test\n",
        "if (EXTRACT_DIR / 'Train').exists():\n",
        "    archive_train = EXTRACT_DIR / 'Train'\n",
        "if (EXTRACT_DIR / 'TestA').exists():\n",
        "    archive_val = EXTRACT_DIR / 'TestA'\n",
        "if (EXTRACT_DIR / 'TestB').exists():\n",
        "    archive_test = EXTRACT_DIR / 'TestB'\n",
        "\n",
        "# Fallbacks: if TestA/TestB not present, we'll split Train folder\n",
        "if archive_train is None:\n",
        "    # find any folder with many images and use it\n",
        "    candidates = [p for p in EXTRACT_DIR.iterdir() if p.is_dir()]\n",
        "    candidates = sorted(candidates, key=lambda p: -sum(1 for f in p.rglob('*') if f.is_file() and f.suffix.lower() in ['.jpg','.png']))\n",
        "    if candidates:\n",
        "        archive_train = candidates[0]\n",
        "\n",
        "print('Archive folders used -> train:', archive_train, 'val:', archive_val, 'test:', archive_test)\n",
        "\n",
        "# Helper to copy class subfolders into our DATA_DIR/split/<classname>/\n",
        "def copy_archive_to_split(archive_root: Path, split_name: str):\n",
        "    if archive_root is None:\n",
        "        return\n",
        "    for class_dir in sorted([p for p in archive_root.iterdir() if p.is_dir()]):\n",
        "        target_dir = DATA_DIR / split_name / class_dir.name\n",
        "        target_dir.mkdir(parents=True, exist_ok=True)\n",
        "        # copy images from possibly nested structure (some archives have Train/Train/Class/*.jpg)\n",
        "        for img in class_dir.rglob('*'):\n",
        "            if img.is_file() and img.suffix.lower() in image_exts:\n",
        "                shutil.copy(img, target_dir / img.name)\n",
        "\n",
        "# If TestA/TestB exist, use them; otherwise split archive_train into train/val/test\n",
        "if archive_val and archive_test:\n",
        "    print('Using Train/TestA/TestB split from archive (no random splitting).')\n",
        "    copy_archive_to_split(archive_train, 'train')\n",
        "    copy_archive_to_split(archive_val, 'val')\n",
        "    copy_archive_to_split(archive_test, 'test')\n",
        "else:\n",
        "    # We'll gather all images under archive_train by class and then do deterministic split per class\n",
        "    print('Creating train/val/test splits by sampling per-class from archive_train.')\n",
        "    for class_dir in sorted([p for p in archive_train.iterdir() if p.is_dir()]):\n",
        "        imgs = [p for p in class_dir.rglob('*') if p.is_file() and p.suffix.lower() in image_exts]\n",
        "        random.shuffle(imgs)\n",
        "        n = len(imgs)\n",
        "        n_train = int(0.7 * n)\n",
        "        n_val = int(0.15 * n)\n",
        "        train_imgs = imgs[:n_train]\n",
        "        val_imgs = imgs[n_train:n_train+n_val]\n",
        "        test_imgs = imgs[n_train+n_val:]\n",
        "        for p in train_imgs:\n",
        "            dest = DATA_DIR / 'train' / class_dir.name\n",
        "            dest.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy(p, dest / p.name)\n",
        "        for p in val_imgs:\n",
        "            dest = DATA_DIR / 'val' / class_dir.name\n",
        "            dest.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy(p, dest / p.name)\n",
        "        for p in test_imgs:\n",
        "            dest = DATA_DIR / 'test' / class_dir.name\n",
        "            dest.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy(p, dest / p.name)\n",
        "\n",
        "# Print counts per split/class\n",
        "from collections import defaultdict\n",
        "counts = defaultdict(int)\n",
        "for split in ['train','val','test']:\n",
        "    for class_dir in (DATA_DIR / split).iterdir() if (DATA_DIR / split).exists() else []:\n",
        "        cnt = sum(1 for f in class_dir.iterdir() if f.is_file())\n",
        "        counts[f\"{split}/{class_dir.name}\"] = cnt\n",
        "\n",
        "print('Counts per split/class (sample):')\n",
        "for k,v in sorted(counts.items())[:50]:\n",
        "    print(f'{k}: {v}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f05021",
      "metadata": {
        "id": "c7f05021",
        "outputId": "ab34286f-990b-4a6e-f1f1-698f74b869fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found classes: ['Train']\n",
            "Detected classes: ['Train']\n"
          ]
        }
      ],
      "source": [
        "# Data transforms and dataloaders\n",
        "from torchvision import datasets, transforms\n",
        "IMG_SIZE = 244\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "data_root = Path('wbc_raabin_split')\n",
        "\n",
        "train_ds = datasets.ImageFolder(data_root/'train', transform=train_tf)\n",
        "val_ds   = datasets.ImageFolder(data_root/'val', transform=val_tf)\n",
        "test_ds  = datasets.ImageFolder(data_root/'test', transform=val_tf)\n",
        "\n",
        "print('Found classes:', train_ds.classes)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Ensure class_names is defined\n",
        "class_names = train_ds.classes\n",
        "print('Detected classes:', class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9d56f0",
      "metadata": {
        "id": "dc9d56f0"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "# Load ResNet50 with proper weights (avoids warnings)\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "model = resnet50(weights=weights)\n",
        "\n",
        "# Replace final layer for WBC classes\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, len(class_names))\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17bb306f",
      "metadata": {
        "id": "17bb306f",
        "outputId": "bd9fa28a-9c98-48f1-ca16-84ed0341921d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - Train loss 0.0000, Train acc 1.0000, Val acc 1.0000\n",
            "Epoch 2/5 - Train loss 0.0000, Train acc 1.0000, Val acc 1.0000\n",
            "Epoch 3/5 - Train loss 0.0000, Train acc 1.0000, Val acc 1.0000\n",
            "Epoch 4/5 - Train loss 0.0000, Train acc 1.0000, Val acc 1.0000\n",
            "Epoch 5/5 - Train loss 0.0000, Train acc 1.0000, Val acc 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Training loop (brief demo)\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        preds = outputs.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(imgs)\n",
        "            preds = outputs.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    val_acc = correct / total\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} - Train loss {train_loss:.4f}, Train acc {train_acc:.4f}, Val acc {val_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a6db29",
      "metadata": {
        "id": "61a6db29",
        "outputId": "6d380adb-7482-4409-f55c-3d83bba1ab33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Train       1.00      1.00      1.00      2119\n",
            "\n",
            "    accuracy                           1.00      2119\n",
            "   macro avg       1.00      1.00      1.00      2119\n",
            "weighted avg       1.00      1.00      1.00      2119\n",
            "\n",
            "Confusion matrix:\n",
            "[[2119]]\n"
          ]
        }
      ],
      "source": [
        "# Final evaluation on test set\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(imgs)\n",
        "        preds = outputs.argmax(1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(all_labels, all_preds, target_names=train_ds.classes))\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ad07a8",
      "metadata": {
        "id": "11ad07a8",
        "outputId": "10195a37-a07a-4ae4-82ba-a5676ebff549"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix Heatmap\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260c59ad",
      "metadata": {
        "id": "260c59ad",
        "outputId": "e3d33854-37d2-4009-d8e5-b06a4e7a31bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No sample images found in test set.\n"
          ]
        }
      ],
      "source": [
        "# Single image inference helper\n",
        "from PIL import Image\n",
        "def predict_image(path):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    x = val_tf(img).unsqueeze(0).to(DEVICE)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        pred = out.argmax(1).item()\n",
        "    return train_ds.classes[pred]\n",
        "\n",
        "# Example: pick a sample from test set (if available)\n",
        "sample = None\n",
        "for cls in train_ds.classes:\n",
        "    p = (data_root/'test'/cls)\n",
        "    if p.exists() and any(p.iterdir()):\n",
        "        sample = next(p.glob('*'))\n",
        "        break\n",
        "if sample:\n",
        "    print('Example sample:', sample)\n",
        "    print('Predicted:', predict_image(sample))\n",
        "else:\n",
        "    print('No sample images found in test set.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19804c9",
      "metadata": {
        "id": "b19804c9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}